# ğŸ“˜ Linear Regression from Scratch â€“ Colab Notebook

A simple implementation of **Linear Regression** using just **NumPy**, written in Python and hosted on **Google Colab** â˜ï¸.  
Perfect for understanding the basics of machine learning and how linear models work under the hood.

---

## ğŸ“Œ What's Inside?

- ğŸ“Š Pure Python + NumPy implementation
- ğŸ§  Gradient Descent-based training
- ğŸ“‰ Visualization of predictions
- ğŸ’¡ Great for learning ML fundamentals

---

## ğŸ§® The Math

We aim to fit a linear model:


yÌ‚ = w * x + b



Where:
- `yÌ‚` is the predicted output
- `w` is the weight (slope)
- `b` is the bias (intercept)

---

### ğŸ¯ Loss Function (Mean Squared Error)

J(w, b) = (1/n) * Î£ (yÌ‚áµ¢ - yáµ¢)Â²

Where:
- `n` is the number of data points
- `yÌ‚áµ¢` is the predicted value
- `yáµ¢` is the actual value

---

### ğŸ” Gradient Descent Updates

w = w - Î± * âˆ‚J/âˆ‚w
b = b - Î± * âˆ‚J/âˆ‚b


Where `Î±` is the learning rate.


---

## â–¶ï¸ Try it on Colab

Click below to open the notebook in Colab:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/CraftyEngineer/Linear-Regression-From-Scratch/blob/main/linear-regression-from-scratch.ipynb)

---

## ğŸ§‘â€ğŸ’» Author

Made with â¤ï¸ by [CraftyEngineer](https://github.com/CraftyEngineer)

---

## ğŸ“œ License

This project is open source and available under the [MIT License](LICENSE).
